library(tigris)
library(ggplot2)
library(acs)
library(data.table)
library(mapproj)
source("functions/theme_map.R")
census_api_key("853b2a1e71aa0aff0f3db966545e8898c73f0772")
year = 2015
span = 5
state_names <- "IA"
county_names <- c("Marshall County")
acs_vars <- c(
"B25070_001","B25070_010",
"B25091_001","B25091_011","B25091_022",
"B25044_001","B25044_003","B25044_010",
"B22010_001","B22010_002",
"B17021_001","B17021_002"
)
acs_est <- get_acs(geography="block group",state=state_names,county=county_names,
variables=acs_vars,year=year,cache_table=TRUE,output="wide", geometry = TRUE)
# convert from sf object to data.table
setDT(acs_est)
View(acs_est)
library(RecordLinkage)
library(stringdist)
# Wikipedia has the following example of the Jaro-distance.
stringdist('MARTHA', 'MATHRA', method='jw')
# Note that stringdist gives a  _distance_ where wikipedia gives the corresponding
# _similarity measure_. To get the wikipedia result:
1 - stringdist('MARTHA', 'MATHRA', method='jw')
# The corresponding Jaro-Winkler distance can be computed by setting p=0.1
stringdist('MARTHA', 'MATHRA', method='jw', p=0.1)
# or, as a similarity measure
1 - stringdist('MARTHA', 'MATHRA', method='jw', p=0.1)
jarowinkler('MARTHA', 'MATHRA')
jarowinkler('MARTHA', 'MATHRA')
# or, as a similarity measure
1 - stringdist('MARTHA', 'MATHRA', method='jw', p=0.1)
# Load data source info
data_info <- data.table::fread("sources/HIFLD.csv")
# For each set of data
for (i in 1:nrow(data_info)) {
# Create schema if needed
schemaname <- data_info[i]$schemaname
con <- sdalr::con_db(dbname = "sdad", host = "localhost", port = 5433, pass = "Iwnftp$2")
DBI::dbGetQuery(con, paste("CREATE SCHEMA IF NOT EXISTS", schemaname))
tablename <- paste0(schemaname, ".", data_info[i]$tablename)
DBI::dbGetQuery(con, paste("TRUNCATE TABLE", tablename))
# # Get Data
# sf::st_write_db(sdalr::con_db("sdal"),
#                 sf::st_read(data_info[i]$url),
#                 table = c(schemaname, data_info[i]$tablename),
#                 row.names = FALSE,
#                 drop = TRUE)
sf::st_write(
sf::st_read(data_info[i]$url),
sdalr::con_db(dbname = "sdad", host = "localhost", port = 5433, pass = "Iwnftp$2"),
layer = c(schemaname, data_info[i]$tablename),
row.names = FALSE,
append = T
)
}
con <- sdalr::con_db(dbname = "sdad", host = "localhost", port = 5433, db_user = Sys.getenv("db_userid"), db_pass = Sys.getenv("db_pwd"))
con <- DBI::dbConnect(
drv = RPostgreSQL::PostgreSQL() ,
dbname = "sdad",
host = "localhost",
port = 5433,
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
# For each set of data
for (i in 1:nrow(data_info)) {
# Create schema if needed
schemaname <- data_info[i]$schemaname
#con <- sdalr::con_db(dbname = "sdad", host = "localhost", port = 5433, db_user = , db_pass = )
con <- DBI::dbConnect(
drv = RPostgreSQL::PostgreSQL() ,
dbname = "sdad",
host = "localhost",
port = 5433,
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbGetQuery(con, paste("CREATE SCHEMA IF NOT EXISTS", schemaname))
tablename <- paste0(schemaname, ".", data_info[i]$tablename)
DBI::dbGetQuery(con, paste("TRUNCATE TABLE", tablename))
# # Get Data
# sf::st_write_db(sdalr::con_db("sdal"),
#                 sf::st_read(data_info[i]$url),
#                 table = c(schemaname, data_info[i]$tablename),
#                 row.names = FALSE,
#                 drop = TRUE)
sf::st_write(
sf::st_read(data_info[i]$url),
sdalr::con_db(dbname = "sdad", host = "localhost", port = 5433, pass = "Iwnftp$2"),
layer = c(schemaname, data_info[i]$tablename),
row.names = FALSE,
append = T
)
}
sf::st_write(
sf::st_read(data_info[i]$url),
DBI::dbConnect(
drv = RPostgreSQL::PostgreSQL() ,
dbname = "sdad",
host = "localhost",
port = 5433,
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd")),
layer = c(schemaname, data_info[i]$tablename),
row.names = FALSE,
append = T
)
# Load data source info
data_info <- data.table::fread("sources/HIFLD.csv")
i=1
# Create schema if needed
schemaname <- data_info[i]$schemaname
con <- DBI::dbConnect(
drv = RPostgreSQL::PostgreSQL() ,
dbname = "sdad",
host = "localhost",
port = 5433,
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbGetQuery(con, paste("CREATE SCHEMA IF NOT EXISTS", schemaname))
tablename <- paste0(schemaname, ".", data_info[i]$tablename)
DBI::dbGetQuery(con, paste("TRUNCATE TABLE", tablename))
DBI::dbSendQuery(con, paste("TRUNCATE TABLE", tablename))
sf::st_write(
sf::st_read(data_info[i]$url),
con,
layer = c(schemaname, data_info[i]$tablename),
row.names = FALSE,
append = T
)
# Load Support Functions
R.utils::sourceDirectory("functions")
# Load data source info
data_info <- data.table::fread("sources/census/census_geospatial_files.csv")
# For each data_info record
for (i in 1:nrow(data_info)) {
pg <- xml2::read_html(data_info[i, url])
links <- rvest::html_attr(rvest::html_nodes(pg, "a"), "href")
ziplinks <- stringr::str_subset(links, ".*zip")
# Download Each Zip and Upload to DB
for (l in ziplinks) {
url <- paste0(data_info[i, url], l)
schemaname <- data_info[i, schemaname]
files <- dataplumbr::file_download_unzip2temp(url)
shpfilepath <- files[grep("\\.shp$", files)[1]]
shpfilebase <- stringr::str_match(basename(shpfilepath), "(.*)\\.shp")[, 2]
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbGetQuery(con, sprintf("CREATE SCHEMA IF NOT EXISTS %s", schemaname))
sf::st_write_db(con,
sf::st_read(shpfilepath),
table = c(schemaname, tolower(shpfilebase)),
row.names = FALSE,
drop = TRUE
)
}
}
i=1
pg <- xml2::read_html(data_info[i, url])
links <- rvest::html_attr(rvest::html_nodes(pg, "a"), "href")
ziplinks <- stringr::str_subset(links, ".*zip")
data_info[i, url]
View(pg)
pg
url="https://www2.census.gov/geo/tiger/GENZ2018/shp/"
u=xml2::read_html(url)
View(u)
u
pg=u
links <- rvest::html_attr(rvest::html_nodes(pg, "a"), "href")
ziplinks <- stringr::str_subset(links, ".*zip")
# Load Support Functions
R.utils::sourceDirectory("functions")
# Load data source info
data_info <- data.table::fread("sources/census/census_geospatial_files.csv")
i=1
pg <- xml2::read_html(data_info[i, url])
links <- rvest::html_attr(rvest::html_nodes(pg, "a"), "href")
ziplinks <- stringr::str_subset(links, ".*zip")
ziplinks
rm(ziplinks)
ziplinks <- stringr::str_subset(links, ".*zip")
rm(links)
rm(ziplinks)
rm(pg)
pg <- xml2::read_html(data_info[i, url])
links <- rvest::html_attr(rvest::html_nodes(pg, "a"), "href")
ziplinks <- stringr::str_subset(links, ".*zip")
l=ziplinks[1]
url <- paste0(data_info[i, url], l)
schemaname <- data_info[i, schemaname]
files <- dataplumbr::file_download_unzip2temp(url)
shpfilepath <- files[grep("\\.shp$", files)[1]]
shpfilebase <- stringr::str_match(basename(shpfilepath), "(.*)\\.shp")[, 2]
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbGetQuery(con, sprintf("CREATE SCHEMA IF NOT EXISTS %s", schemaname))
sf::st_write_db(con,
sf::st_read(shpfilepath),
table = c(schemaname, tolower(shpfilebase)),
row.names = FALSE,
drop = TRUE
)
# Load Support Functions
R.utils::sourceDirectory("functions")
install.packages("tidycensus")
# Load Support Functions
R.utils::sourceDirectory("functions")
install.packages("plumber")
# Load Support Functions
R.utils::sourceDirectory("functions")
# Load data source info
data_info <- data.table::fread("sources/census/census_geospatial_files.csv")
# For each data_info record
for (i in 1:nrow(data_info)) {
pg <- xml2::read_html(data_info[i, url])
links <- rvest::html_attr(rvest::html_nodes(pg, "a"), "href")
ziplinks <- stringr::str_subset(links, ".*zip")
# Download Each Zip and Upload to DB
for (l in ziplinks) {
url <- paste0(data_info[i, url], l)
schemaname <- data_info[i, schemaname]
files <- dataplumbr::file_download_unzip2temp(url)
shpfilepath <- files[grep("\\.shp$", files)[1]]
shpfilebase <- stringr::str_match(basename(shpfilepath), "(.*)\\.shp")[, 2]
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbGetQuery(con, sprintf("CREATE SCHEMA IF NOT EXISTS %s", schemaname))
sf::st_write_db(con,
sf::st_read(shpfilepath),
table = c(schemaname, tolower(shpfilebase)),
row.names = FALSE,
drop = TRUE
)
}
}
exists <- DBI::dbGetQuery(con, sprintf("SELECT * FROM information_schema WHERE table_name = '%s'", tolower(shpfilebase)))
exists <- DBI::dbGetQuery(con, sprintf("SELECT * FROM information_schema.tables WHERE table_name = '%s'", tolower(shpfilebase)))
View(exists)
# For each data_info record
for (i in 1:nrow(data_info)) {
pg <- xml2::read_html(data_info[i, url])
links <- rvest::html_attr(rvest::html_nodes(pg, "a"), "href")
ziplinks <- stringr::str_subset(links, ".*zip")
# Download Each Zip and Upload to DB
for (l in ziplinks) {
url <- paste0(data_info[i, url], l)
schemaname <- data_info[i, schemaname]
files <- dataplumbr::file_download_unzip2temp(url)
shpfilepath <- files[grep("\\.shp$", files)[1]]
shpfilebase <- stringr::str_match(basename(shpfilepath), "(.*)\\.shp")[, 2]
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbGetQuery(con, sprintf("CREATE SCHEMA IF NOT EXISTS %s", schemaname))
exists <- DBI::dbGetQuery(con, sprintf("SELECT * FROM information_schema.tables WHERE table_name = '%s'", tolower(shpfilebase)))
if (nrow(exists) > 0) {
sf::st_write_db(con,
sf::st_read(shpfilepath),
table = c(schemaname, tolower(shpfilebase)),
row.names = FALSE,
drop = TRUE
)
}
DBI::dbDisconnect(con)
}
}
# For each data_info record
for (i in 1:nrow(data_info)) {
pg <- xml2::read_html(data_info[i, url])
links <- rvest::html_attr(rvest::html_nodes(pg, "a"), "href")
ziplinks <- stringr::str_subset(links, ".*zip")
# Download Each Zip and Upload to DB
for (l in ziplinks) {
url <- paste0(data_info[i, url], l)
schemaname <- data_info[i, schemaname]
files <- dataplumbr::file_download_unzip2temp(url)
shpfilepath <- files[grep("\\.shp$", files)[1]]
shpfilebase <- stringr::str_match(basename(shpfilepath), "(.*)\\.shp")[, 2]
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbGetQuery(con, sprintf("CREATE SCHEMA IF NOT EXISTS %s", schemaname))
exists <- DBI::dbGetQuery(con, sprintf("SELECT * FROM information_schema.tables WHERE table_name = '%s'", tolower(shpfilebase)))
if (nrow(exists) == 0) {
sf::st_write_db(con,
sf::st_read(shpfilepath),
table = c(schemaname, tolower(shpfilebase)),
row.names = FALSE,
drop = TRUE
)
}
DBI::dbDisconnect(con)
}
}
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbListTables(con)
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbListObjects(con)
DBI::dbDisconnect(con)
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
#DBI::dbListObjects(con)
DBI::dbGetQuery(con, "SELECT table_schema, table_name FROM information_schema ORDER BY table_schema, table_name")
DBI::dbDisconnect(con)
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
#DBI::dbListObjects(con)
DBI::dbGetQuery(con, "SELECT table_schema, table_name FROM information_schema.tables ORDER BY table_schema, table_name")
DBI::dbDisconnect(con)
data_info <- data.table::fread("sources/census/census_geospatial_files.csv")
# For each data_info record
for (i in 1:nrow(data_info)) {
pg <- xml2::read_html(data_info[i, url])
links <- rvest::html_attr(rvest::html_nodes(pg, "a"), "href")
ziplinks <- stringr::str_subset(links, ".*zip")
# Download Each Zip and Upload to DB
for (l in ziplinks) {
url <- paste0(data_info[i, url], l)
schemaname <- data_info[i, schemaname]
files <- dataplumbr::file_download_unzip2temp(url)
shpfilepath <- files[grep("\\.shp$", files)[1]]
shpfilebase <- stringr::str_match(basename(shpfilepath), "(.*)\\.shp")[, 2]
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbGetQuery(con, sprintf("CREATE SCHEMA IF NOT EXISTS %s", schemaname))
exists <- DBI::dbGetQuery(con, sprintf("SELECT * FROM information_schema.tables WHERE table_name = '%s'", tolower(shpfilebase)))
if (nrow(exists) == 0) {
sf::st_write_db(con,
sf::st_read(shpfilepath),
table = c(schemaname, tolower(shpfilebase)),
row.names = FALSE,
drop = TRUE
)
}
DBI::dbDisconnect(con)
}
}
data_info <- data.table::fread("sources/census/census_geospatial_files.csv")
# For each data_info record
for (i in 1:nrow(data_info)) {
pg <- xml2::read_html(data_info[i, url])
links <- rvest::html_attr(rvest::html_nodes(pg, "a"), "href")
ziplinks <- stringr::str_subset(links, ".*zip")
# Download Each Zip and Upload to DB
for (l in ziplinks) {
url <- paste0(data_info[i, url], l)
schemaname <- data_info[i, schemaname]
files <- dataplumbr::file_download_unzip2temp(url)
shpfilepath <- files[grep("\\.shp$", files)[1]]
shpfilebase <- stringr::str_match(basename(shpfilepath), "(.*)\\.shp")[, 2]
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbGetQuery(con, sprintf("CREATE SCHEMA IF NOT EXISTS %s", schemaname))
exists <- DBI::dbGetQuery(con, sprintf("SELECT * FROM information_schema.tables WHERE table_name = '%s'", tolower(shpfilebase)))
if (nrow(exists) == 0) {
sf::st_write_db(con,
sf::st_read(shpfilepath),
table = c(schemaname, tolower(shpfilebase)),
row.names = FALSE,
drop = TRUE
)
}
DBI::dbDisconnect(con)
}
}
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
mygeo <- sf::st_read(con, c("census_cb", "cb_2018_19_county_within_ua_500k"))
plot(mygeo[, c("ALAND10")])
url="https://www2.census.gov/geo/tiger/GENZ2016/kml/cb_2016_us_county_500k.zip"
files <- dataplumbr::file_download_unzip2temp(url)
shpfilepath <- files[grep("\\.shp$", files)[1]]
shpfilebase <- stringr::str_match(basename(shpfilepath), "(.*)\\.shp")[, 2]
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbGetQuery(con, sprintf("CREATE SCHEMA IF NOT EXISTS %s", schemaname))
if (nrow(exists) == 0) {
sf::st_write_db(con,
sf::st_read(shpfilepath),
table = c(schemaname, tolower(shpfilebase)),
row.names = FALSE,
drop = TRUE
)
}
DBI::dbDisconnect(con)
shpfilepath
files
url="https://www2.census.gov/geo/tiger/GENZ2016/shp/cb_2016_us_county_500k.zip"
files <- dataplumbr::file_download_unzip2temp(url)
shpfilepath <- files[grep("\\.shp$", files)[1]]
shpfilebase <- stringr::str_match(basename(shpfilepath), "(.*)\\.shp")[, 2]
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbGetQuery(con, sprintf("CREATE SCHEMA IF NOT EXISTS %s", schemaname))
if (nrow(exists) == 0) {
sf::st_write_db(con,
sf::st_read(shpfilepath),
table = c(schemaname, tolower(shpfilebase)),
row.names = FALSE,
drop = TRUE
)
}
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
mygeo <- sf::st_read(con, c("census_cb", "cb_2016_us_county_500k"))
plot(mygeo[, c("ALAND")])
plot(mygeo[mygeo$STATEFP=="51", c("ALAND")])
plot(mygeo[mygeo$STATEFP %in% c("51, 54"), c("ALAND")])
plot(mygeo[mygeo$STATEFP %in% c("51", "54"), c("ALAND")])
plot(mygeo[mygeo$STATEFP %in% c("51", "54", "24"), c("ALAND")])
## CENSUS GeoSpatial Files Virginia
# Load Support Functions
R.utils::sourceDirectory("functions")
# Load data source info
data_info <- data.table::fread("sources/census/census_geospatial_files.csv")
# For each data_info record
for (i in 1:nrow(data_info)) {
pg <- xml2::read_html(data_info[i, url])
links <- rvest::html_attr(rvest::html_nodes(pg, "a"), "href")
ziplinks <- stringr::str_subset(links, ".*zip")
# Download Each Zip and Upload to DB
for (l in ziplinks) {
url <- paste0(data_info[i, url], l)
schemaname <- data_info[i, schemaname]
files <- dataplumbr::file_download_unzip2temp(url)
shpfilepath <- files[grep("\\.shp$", files)[1]]
shpfilebase <- stringr::str_match(basename(shpfilepath), "(.*)\\.shp")[, 2]
con <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
dbname = "gis",
host = "postgis_1",
port = "5432",
user = Sys.getenv("db_userid"),
password = Sys.getenv("db_pwd"))
DBI::dbGetQuery(con, sprintf("CREATE SCHEMA IF NOT EXISTS %s", schemaname))
exists <- DBI::dbGetQuery(con, sprintf("SELECT * FROM information_schema.tables WHERE table_name = '%s'", tolower(shpfilebase)))
if (nrow(exists) == 0) {
sf::st_write_db(con,
sf::st_read(shpfilepath),
table = c(schemaname, tolower(shpfilebase)),
row.names = FALSE,
drop = TRUE
)
}
DBI::dbDisconnect(con)
}
}
